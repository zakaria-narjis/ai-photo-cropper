{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import cv2\n",
    "import json\n",
    "from CACNet import CACNet\n",
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from io import BytesIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory_model_size = torch.cuda.max_memory_allocated()/1024/1024\n",
    "print(f'{memory_model_size}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "DEVICE.type=='cuda'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_SIZE = (224,224)\n",
    "IMAGE_NET_MEAN = [0.485, 0.456, 0.406]\n",
    "IMAGE_NET_STD = [0.229, 0.224, 0.225]\n",
    "\n",
    "class ClearCache:\n",
    "    def __enter__(self):\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    def __exit__(self, exc_type, exc_val, exc_tb):\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "class ImagesDataset(Dataset):\n",
    "    def __init__(self, images_as_bytes:list[bytes]) :\n",
    "        self.images_as_bytes = images_as_bytes \n",
    "        # self.resized_image,self.im_widths,self.im_heights  = self.process_images(images_as_bytes)  \n",
    "        self.IMAGE_SIZE = torch.tensor(IMAGE_SIZE, requires_grad=False)\n",
    "        self.image_transformer = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=IMAGE_NET_MEAN, std=IMAGE_NET_STD)])\n",
    "\n",
    "    def  process_images(self,image):\n",
    "        image = Image.open(BytesIO(image)).convert('RGB')\n",
    "        im_width, im_height = image.size\n",
    "        h = self.IMAGE_SIZE[0]\n",
    "        w = self.IMAGE_SIZE[1]\n",
    "        resized_image = image.resize((w, h), Image.LANCZOS)\n",
    "        im_widths = torch.tensor(im_widths, requires_grad=False)\n",
    "        im_heights = torch.tensor(im_heights, requires_grad=False)\n",
    "        resized_image = self.image_transformer(resized_image)\n",
    "        resized_image.requires_grad=False\n",
    "        return  resized_image, im_width, im_height  \n",
    "    \n",
    "    def __len__(self,):\n",
    "        return len(self.images_as_bytes)\n",
    "    \n",
    "    def __getitem__(self,index):\n",
    "        return self.process_images(self.images_as_bytes[index])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess as sp\n",
    "import os\n",
    "\n",
    "def get_gpu_memory():\n",
    "    command = \"nvidia-smi --query-gpu=memory.free --format=csv\"\n",
    "    memory_free_info = sp.check_output(command.split()).decode('ascii').split('\\n')[:-1][1:]\n",
    "    memory_free_values = [int(x.split()[0]) for i, x in enumerate(memory_free_info)]\n",
    "    return memory_free_values\n",
    "\n",
    "get_gpu_memory()[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(get_gpu_memory()[0]*1024*1024)/(3*4*224*224*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=[1]\n",
    "b=[2]\n",
    "a.extend(b)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_file = \"./pretrained_models/best-FLMS_iou.pth\"\n",
    "model = CACNet(loadweights=False)\n",
    "model.load_state_dict(torch.load(weight_file,map_location=device))\n",
    "model = model.to(device).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_NET_MEAN = [0.485, 0.456, 0.406]\n",
    "IMAGE_NET_STD = [0.229, 0.224, 0.225]\n",
    "IMAGE_SIZE = (224,224)\n",
    "def parse_annotation(index):\n",
    "    data_file = os.path.join(cfg.FCDB_dir, 'cropping_testing_set.json')\n",
    "    origin_data = json.loads(open(data_file, 'r').read())\n",
    "    image_data = origin_data[index]\n",
    "    image = os.path.split(image_data['url'])[-1]\n",
    "    x,y,w,h = image_data['crop']\n",
    "    gt_crop= [x,y,x+w,y+h]\n",
    "    gt_crop = np.array(gt_crop).reshape(-1,4).astype(np.float32)\n",
    "    return image,gt_crop\n",
    "    \n",
    "def process_image(image_file):\n",
    "    image = Image.open(image_file).convert('RGB')\n",
    "    im_width, im_height = image.size\n",
    "    h = IMAGE_SIZE[1]\n",
    "    w = IMAGE_SIZE[0]\n",
    "    resized_image = image.resize((w, h), Image.LANCZOS)\n",
    "    image_transformer = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=IMAGE_NET_MEAN, std=IMAGE_NET_STD)])\n",
    "    im = image_transformer(resized_image)\n",
    "    return im, im_width, im_height, image_file\n",
    "\n",
    "def predict(image,model = model):\n",
    "    logits,kcm,crop = model(image, only_classify=False)\n",
    "    return logits,kcm,crop\n",
    "\n",
    "def crop_image(image):\n",
    "    im, im_width, im_height, image_file = process_image(image)\n",
    "    if im.dim()<4:\n",
    "        im = im.unsqueeze(0)\n",
    "    im = im.to(device)\n",
    "    logits,kcm,crop = predict(im)\n",
    "    print(im.shape)\n",
    "    crop[:,0::2] = crop[:,0::2] / im.shape[-1] * im_width\n",
    "    crop[:,1::2] = crop[:,1::2] / im.shape[-2] * im_height\n",
    "    pred_crop = crop.detach().cpu()\n",
    "    pred_crop[:,0::2] = torch.clip(pred_crop[:,0::2], min=0, max=im_width)\n",
    "    pred_crop[:,1::2] = torch.clip(pred_crop[:,1::2], min=0, max=im_height)\n",
    "    pred_crop = pred_crop[0].numpy().tolist()\n",
    "    x1,y1,x2,y2 = [int(x) for x in pred_crop]\n",
    "    source_img = cv2.imread(image_file)\n",
    "    source_img = cv2.rectangle(source_img , (x1,y1), (x2,y2), (255,0,0), 2) \n",
    "    return source_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(id(IMAGE_SIZE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_file,gt_crop = parse_annotation(3)\n",
    "x1,y1,x2,y2 = [int(x) for x in gt_crop[0]]\n",
    "image_file = os.path.join(cfg.FCDB_dir,f'data/{image_file}')\n",
    "image_file = crop_image(image_file)\n",
    "image_file = cv2.rectangle(image_file , (x1,y1), (x2,y2), (0,255,0), 2) \n",
    "import matplotlib.pyplot as plt \n",
    "plt.imshow(image_file)\n",
    "print(image_file.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from crop import Cropper\n",
    "image_file,gt_crop = parse_annotation(3)\n",
    "image_file = os.path.join(cfg.FCDB_dir,f'data/{image_file}')\n",
    "crop_model = Cropper()\n",
    "img,x1,y1,x2,y2 = crop_model.crop_image(image_file)\n",
    "import matplotlib.pyplot as plt \n",
    "plt.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image, ImageDraw \n",
    "import numpy as np\n",
    "img = Image.open('sample.jpg' )\n",
    "draw = ImageDraw.Draw(img.copy())\n",
    "bbox = {\n",
    "  \"x1\": 7,\n",
    "  \"y1\": 38,\n",
    "  \"x2\": 1019,\n",
    "  \"y2\": 567\n",
    "}\n",
    "draw.rectangle(((bbox[\"x1\"], bbox[\"y1\"]),(bbox[\"x2\"], bbox[\"y2\"])), outline='Red')\n",
    "cropped_image = img.crop(list(bbox.values()))\n",
    "import matplotlib.pyplot as plt \n",
    "# print(np.array(img))\n",
    "plt.imshow(cropped_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "a=io.BytesIO()\n",
    "np.save(a, img)\n",
    "# print(a)\n",
    "# b = np.load(io.BytesIO(a.getvalue()))\n",
    "Image.open(a).convert('RGB')\n",
    "\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
